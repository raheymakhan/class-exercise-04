{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/In_class_exercise/In_class_exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EuX00KHNeSpw"
   },
   "source": [
    "# **The fourth in-class-exercise (20 points in total)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-vTOb03hG1f"
   },
   "source": [
    "# 1. Text Data Preprocessing\n",
    "\n",
    "Here is a [legal case](https://github.com/unt-iialab/INFO5731_FALL2020/blob/master/In_class_exercise/01-05-1%20%20Adams%20v%20Tanner.txt) we collected from westlaw, please follow the steps we mentioned in lesson 5 to clean the data:\n",
    "\n",
    "\n",
    "\n",
    "## 1.1 Basic feature extraction using text data (4 points)\n",
    "\n",
    "*   Number of sentences\n",
    "*   Number of words\n",
    "*   Number of characters\n",
    "*   Average word length\n",
    "*   Number of stopwords\n",
    "*   Number of special characters\n",
    "*   Number of numerics\n",
    "*   Number of uppercase words\n",
    "\n",
    "## 1.2 Basic Text Pre-processing of text data (4 points)\n",
    "\n",
    "*   Lower casing\n",
    "*   Punctuation removal\n",
    "*   Stopwords removal\n",
    "*   Frequent words removal\n",
    "*   Rare words removal\n",
    "*   Spelling correction\n",
    "*   Tokenization\n",
    "*   Stemming\n",
    "*   Lemmatization\n",
    "\n",
    "## 1.3 Save all the **clean sentences** to a **csv file** (one column, each raw is a sentence) after finishing all the steps above. (4 points)\n",
    "\n",
    "\n",
    "## 1.4 Advance Text Processing (Extra credit: 4 points)\n",
    "\n",
    "*   Calculate the term frequency of all the terms.\n",
    "*   Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vR0L3_CreM_A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sentences is: 148\n",
      "The number of words is: 3722\n",
      "The number of characters is: 20295\n",
      "The average word length is: 4.510385756676558\n",
      "The number of stopwords is: 1679\n",
      "The number of special characters is: 13\n",
      "The number of numerics is: 60\n",
      "The number of uppercase words is: 84\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "# 1.1 BASIC FEATURE EXTRACTION USING TEXT DATA\n",
    "\n",
    "file = r\"C:\\Users\\Raheyma Arshad\\Desktop\\Legal Case - Class Exercise 03.txt\"\n",
    "f = open(file, encoding=\"utf8\")\n",
    "text = f.read().splitlines()\n",
    "while(\"\" in text) : \n",
    "    text.remove(\"\") \n",
    "\n",
    "# Number of sentences\n",
    "number_of_sentences = len(text)\n",
    "print('The number of sentences is:', number_of_sentences)\n",
    "\n",
    "# Number of words\n",
    "word_count = len(str(text).split(\" \"))\n",
    "print ('The number of words is:', word_count)\n",
    "\n",
    "# Number of characters\n",
    "character_count = sum(len(i) for i in text)\n",
    "print ('The number of characters is:', character_count)\n",
    "\n",
    "# Average word length\n",
    "words = []\n",
    "for sentence in text:\n",
    "    words.append(sentence.split())\n",
    "\n",
    "words_list = []\n",
    "for sublist in words:\n",
    "    for item in sublist:\n",
    "        words_list.append(item)\n",
    "\n",
    "each_word_length = []\n",
    "for word in words_list:\n",
    "    each_word_length.append(len(word))\n",
    "\n",
    "avg_word_length = sum(each_word_length)/len(each_word_length)\n",
    "print ('The average word length is:', avg_word_length)\n",
    "\n",
    "# Number of stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "stopwords = []\n",
    "for word in words_list:\n",
    "    if word in stop:\n",
    "        stopwords.append(word)\n",
    "        \n",
    "number_of_stopwords = len(stopwords)\n",
    "print ('The number of stopwords is:', number_of_stopwords)\n",
    "\n",
    "# Number of special characters\n",
    "special_characters = []\n",
    "for word in words_list:\n",
    "    if word.startswith('*'):\n",
    "        special_characters.append(word)\n",
    "        \n",
    "number_of_special_characters = len(special_characters)\n",
    "print ('The number of special characters is:', number_of_special_characters)\n",
    "\n",
    "# Number of numerics\n",
    "numerics = []\n",
    "for word in words_list:\n",
    "    if word.isdigit():\n",
    "        numerics.append(word)\n",
    "        \n",
    "number_of_numerics = len(numerics)\n",
    "print ('The number of numerics is:', number_of_numerics)\n",
    "\n",
    "# Number of uppercase words\n",
    "uppercase = []\n",
    "for word in words_list:\n",
    "    if word.isupper():\n",
    "        uppercase.append(word)\n",
    "        \n",
    "number_of_uppercase = len(uppercase)\n",
    "print ('The number of uppercase words is:', number_of_uppercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [5, all, 740]\n",
       "1     [suprem, alabama]\n",
       "2                [adam]\n",
       "4      [manner, norton]\n",
       "5    [june, term, 1843]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################################################################################################################\n",
    "\n",
    "# 1.2 BASIC TEXT PRE-PROCESSING OF TEXT DATA\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.DataFrame(text)\n",
    "data.columns = ['text']\n",
    "data['text'].replace('', np.nan, inplace=True)\n",
    "data.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "# Lower casing\n",
    "data['text'] = data['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "# Punctuation removal\n",
    "data['text'] = data['text'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Stopwords removal\n",
    "data['text'] = data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "# Frequent words removal\n",
    "high_freq = pd.Series(' '.join(data['text']).split()).value_counts()[:10]\n",
    "high_freq = list(high_freq.index)\n",
    "data['text'] = data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in high_freq))\n",
    "\n",
    "# Rare words removal\n",
    "low_freq = pd.Series(' '.join(data['text']).split()).value_counts()[-10:]\n",
    "low_freq = list(low_freq.index)\n",
    "data['text'] = data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in low_freq))\n",
    "\n",
    "# Spelling correction\n",
    "from textblob import TextBlob\n",
    "data['text'] = data['text'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "\n",
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "data['text'] = data['text'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "# Lemmatization\n",
    "from textblob import Word\n",
    "data['text'] = data['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "data['text'].replace('', np.nan, inplace=True)\n",
    "data.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "# Tokenization\n",
    "data['text'] = data['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################################\n",
    "\n",
    "# 1.3 SAVE ALL THE CLEAN SENTENCES TO A CSV FILE \n",
    "\n",
    "data.to_csv(r\"C:\\Users\\Raheyma Arshad\\Desktop\\Clean Sentences.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       words    tf\n",
      "0        all  17.0\n",
      "1          5   8.0\n",
      "2        740   2.0\n",
      "3    alabama   1.0\n",
      "4     suprem   1.0\n",
      "..       ...   ...\n",
      "700      rye   1.0\n",
      "701     half   1.0\n",
      "702     file   2.0\n",
      "703      neg   2.0\n",
      "704  histori   2.0\n",
      "\n",
      "[705 rows x 2 columns]\n",
      "The top 10 1-gram are:\n",
      "[('firm',), ('condit',), ('major',), ('seen',), ('wm',), ('meant',), ('singular',), ('inclin',), ('writ',), ('point',)]\n",
      "The top 10 2-gram are:\n",
      "[('746', 'seen'), ('statut', 'design'), ('septemb', 'control'), ('confirm', 'correct'), ('sold', 'assumpt'), ('point', 'inquiri'), ('all', '464'), ('attach', 'upon'), ('fact', 'state'), ('offic', 'levi')]\n",
      "The top 10 3-gram are:\n",
      "[('act', '1821', 'ask'), ('power', 'take', 'posse'), ('u', 'back', 'question'), ('cotton', 'count', 'appeal'), ('bill', 'except', 'seal'), ('must', 'may', '1840'), ('upon', 'subject', 'polici'), ('defend', 'might', 'time'), ('accord', 'evid', 'ad'), ('learn', 'comment', 'doubt')]\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################################\n",
    "\n",
    "# 1.4 ADVANCE TEXT PROCESSING\n",
    "\n",
    "# Calculate the term frequency of all the terms.\n",
    "term_freq = (data['text']).apply(lambda x: pd.value_counts(x)).sum(axis = 0).reset_index()\n",
    "term_freq.columns = ['words', 'tf']\n",
    "print(term_freq)\n",
    "\n",
    "# Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n",
    "data['text_1'] = [','.join(map(str, l)) for l in data['text']]\n",
    "\n",
    "# Top 10 1-gram\n",
    "from nltk.util import ngrams\n",
    "data['1-grams'] = data['text'].apply(lambda row: list(nltk.ngrams(row, 1)))\n",
    "gram_1 = list(set([a for b in data['1-grams'].tolist() for a in b]))\n",
    "\n",
    "gram_1_counter = {}\n",
    "for gram in gram_1:\n",
    "    if gram in gram_1_counter:\n",
    "        gram_1_counter[gram] += 1\n",
    "    else:\n",
    "        gram_1_counter[gram] = 1\n",
    "\n",
    "sorted_gram_1 = sorted(gram_1_counter, key = gram_1_counter.get, reverse = True)\n",
    "top_10_gram_1 = sorted_gram_1[:10]\n",
    "\n",
    "print('The top 10 1-gram are:')\n",
    "print(top_10_gram_1)\n",
    "\n",
    "# Top 10 2-grams\n",
    "data['2-grams'] = data['text'].apply(lambda row: list(nltk.ngrams(row, 2)))\n",
    "gram_2 = list(set([a for b in data['2-grams'].tolist() for a in b]))\n",
    "\n",
    "gram_2_counter = {}\n",
    "for gram in gram_2:\n",
    "    if gram in gram_2_counter:\n",
    "        gram_2_counter[gram] += 1\n",
    "    else:\n",
    "        gram_2_counter[gram] = 1\n",
    "\n",
    "sorted_gram_2 = sorted(gram_2_counter, key = gram_2_counter.get, reverse = True)\n",
    "top_10_gram_2 = sorted_gram_2[:10]\n",
    "\n",
    "print('The top 10 2-gram are:')\n",
    "print(top_10_gram_2)\n",
    "\n",
    "# Top 10 3-grams\n",
    "data['3-grams'] = data['text'].apply(lambda row: list(nltk.ngrams(row, 3)))\n",
    "gram_3 = list(set([a for b in data['3-grams'].tolist() for a in b]))\n",
    "\n",
    "gram_3_counter = {}\n",
    "for gram in gram_3:\n",
    "    if gram in gram_3_counter:\n",
    "        gram_3_counter[gram] += 1\n",
    "    else:\n",
    "        gram_3_counter[gram] = 1\n",
    "\n",
    "sorted_gram_3 = sorted(gram_3_counter, key = gram_3_counter.get, reverse = True)\n",
    "top_10_gram_3 = sorted_gram_3[:10]\n",
    "\n",
    "print('The top 10 3-gram are:')\n",
    "print(top_10_gram_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BBiC4E_kefvV"
   },
   "source": [
    "# 2. Python Regular Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z1QJ-UwCenvN"
   },
   "source": [
    "## 2.1 Write a Python program to remove leading zeros from an IP address. (4 points)\n",
    "\n",
    "ip = \"260.08.094.109\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wSv6fVhOfFmv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'260.9.94.109'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_zeros(ip): \n",
    "    new_ip = re.sub('\\.[0]*', '.', ip)\n",
    "    return new_ip\n",
    "    \n",
    "      \n",
    "remove_zeros(\"260.09.094.109\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXRjaHzrfKAy"
   },
   "source": [
    "## 2.2 Write a Python Program to extract all the years from the following sentence. (4 points)\n",
    "\n",
    "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7xdJpDx9gjbX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2010, 2019]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_years(text):\n",
    "    # Find all four digit numbers\n",
    "    four_digit_numbers = re.findall(r'(\\d{4})', text)\n",
    "    # Convert them to integers\n",
    "    four_digit_numbers = [int(i) for i in four_digit_numbers] \n",
    "    # Remove numbers below 1500 because usually they do not represent years \n",
    "    for i in four_digit_numbers:\n",
    "        if i < 1500:\n",
    "            four_digit_numbers.remove(i)\n",
    "    # Remove any duplicates\n",
    "    years = list(dict.fromkeys(four_digit_numbers))\n",
    "    return years\n",
    "        \n",
    "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\"\n",
    "\n",
    "extract_years(sentence)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPBbHN93zlE4UST77Gx/JWu",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "In-class-exercise-04.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
